@meta

```json
{
	"id": "mit6172-4",
	"createTime": "2025-09-17 10:59",
	"background": "http://pic.caiwen.work/i/2025/09/21/68cf4c541ab39.png",
	"key": [
		"mit6.172",
		"软件性能工程",
		"优化",
		"并行",
		"多核",
		"多线程",
		"调度",
		"work stealing"
	]
}
```

## 1. 并行方案

### 1.1 Pthread

具体参考 CSAPP 的笔记：TBD

创建的线程将由操作系统内核进行调度。使用线程的好处就是可以灵活地进行控制，缺点就是创建线程的开销太大（可以使用线程池解决，即提前创建好若干线程放到线程池中等待使用。我们不断地去复用线程池中的线程，避免了来回创建又销毁的开销），而且需要人为控制，容易出问题。

### 1.2 Threading Building Blocks

这是由 Intel 主导的一个多任务并行的库。

我个人感觉这个有点像 Rust 的 Tokio。大概思想可以理解为就是他内部有个线程池（线程池中线程的个数一般和 CPU 核心数量差不多，即一个核心上跑一个线程）。

我们把需要并行的任务当作一个“轻量级的协程”。TBB 将在运行时决定把一个任务放到哪个 CPU 的核上去运行。和单纯的线程相比，创建任务的开销很小，并且任务之间上下文的切换的开销也比较小。同时，TBB 使用了 work stealing 的任务调度算法，使得其可以充分利用 CPU 各个核。

大概的语法如下：

![](http://pic.caiwen.work/i/2025/09/17/68ca51c1b2d8c.png)

值得一提的是，`set_ref_count` 设置当前任务数量，应设置为要 spawn 出去的子任务数量再加上 1（即自己本身）。然后后面的 `spawn_and_wait_for_all` 就是根据前面设置的 `set_ref_count` 来判断是不是已经 wait 掉所有任务了。

### 1.3 OpenMP

OpenMP 相当于是一个语法层面的扩展，目前很多编译器已经支持。在编译时添加 `-fopenmp` 即可启用对 OpenMP 的语法解析，并且编译器会链接 OpenMP 的库。

感觉 OpenMP 和 TBB 一样，就是把线程进行了封装，在运行时进行调度。

不过 OpenMP 的一个优点时，适合快速将已有的代码变成并行的，更简洁一点。

![](http://pic.caiwen.work/i/2025/09/17/68ca546e34b63.png)

## 2. 并行度

我们可以根据程序的过程得到一个 DAG：

![](http://pic.caiwen.work/i/2025/09/17/68ca5740c2f60.png)

其中一个点连出去多个点，意味着这些点都可以并行地计算。上图标号为 4 的框中，紫色的点连出去另一个紫色的点，表示 spawn 出去一个任务，连出去绿色的点表示 spawn 完任务之后自己继续执行的代码。最后经过 `cilk_sync` 同步之后会来到蓝色的那个点。

DAG 上的每一个点都被称为一个 strand，表示不存在 spawn、sync、call、return 这些的指令序列。同时，入度为 0 的点称为 initial strand，出度为 0 的点称为 final strand。同时各个边也根据行为有不同的名称：

![](http://pic.caiwen.work/i/2025/09/17/68ca591832d2a.png)

我们再看一个 DAG：

![](http://pic.caiwen.work/i/2025/09/17/68ca59ed0a271.png)

假定 DAG 上每个点都耗时一个单位时间。

定义 **work** 表示总工作量，记为 $T_1$，为所有点的数量。也可以理解为是只使用单核计算的耗时。在这个 DAG 中 $T_1=18$。

定义 **span** 表示 DAG 上的最长点数路径的长度，记为 $T_{\infin}$。也可以理解为假如我们有无穷多个 CPU 核心进行计算的耗时。我们也称 span 为 **critical-path length** 或是 **computational depth**。在这个 DAG 中 $T_{\infin}=9$。

令 $P$ 表示 CPU 核心数量，$T_P$ 表示利用 P 个 CPU 核心并行计算所耗费时间，我们有：

**Work Law**：$T_P\ge \frac{T_1}{P}$。可以认为，我们把全部的 work 都分到所有的核上，这样最少也是 $\frac{T_1}{P}$ 的时间。而考虑到分配的开销，以及可能调度并没有使所有核都一直处于工作状态，所以时间会有所增加。

**Span Law**：$T_P \ge T_{\infin}$。

定义 **speed up** 为 $\frac{T_1}{T_P}$。

- 如果 $\frac{T_1}{T_P} < P$，我们就得到了一个 **sublinear speedup**。
- 如果 $\frac{T_1}{T_P} = P$，我们就得到了一个 **linear speedup**。
- 如果 $\frac{T_1}{T_P} > P$，我们就得到了一个 **superlinear speedup**。在我们目前的简单模型中是不会出现的，因为这和 Work Law 是矛盾的。但是实际情况中会出现，这是因为，我们目前隐含的前提是，并行运行和串行运行时，一个任务的耗时是不变的。而实际上可能会出现这样一种情况，比如我们要处理一个很大的数据，直接串行的话可能会使得缓存命中率不高。而如果把数据切分成小块，在不同的 CPU 核心上并行运行时，可能处理的数据大小恰好能放入缓存中，使得缓存命中率增高，因此耗时变短。

定义 **parallelism** 为我们能获得的最大的 speedup，也称为并行度。根据 Span Law，parallelism 就等于 $\frac{T_1}{T_{\infin}}$。

## 3. 并行问题

### 3.1 缓存一致性

CPU 中每个核有自己的 L1 和 L2 缓存，然后又有共享的 L3 缓存。这样一来，就可能出现缓存一致性的问题，比如一个核将某个地址的数据缓存到自己的 L2 缓存中，另一个核又把这个地址的数据修改了，而前者的 L2 缓存此时并没有更新，这就出现了不一致的问题。

为了使得多核之间的缓存保持一致，我们有 MSI 协议，每个缓存行都有如下的三种状态：

- M：当前缓存行中的缓存块被修改了。MSI 协议要求此时不存在其他的核，使得该缓存行处于 M 或是 S 状态。
- S：其他的核中可能也存在这个缓存行，这个缓存行是被共享的。
- I：当前缓存行是无效的。

那么每当一个核要对数据进行修改时，除了修改自己核的缓存中的数据之外，还把当前缓存行状态置为 M，并将其他核中的该缓存行（如果有）状态置为 I。

### 3.2 Race/Reducer

determinacy race 的原因什么的就不多说了。

值得一提的是，结构体也可能会出现 determinacy race，如：

```cpp
struct {
    char a;
    char b;
} x;
```

如果一个线程修改 `x.a`，一个线程修改 `x.b`，在一些架构或是编译优化行为上，可能会出现更新将这个结构体看成是 16 位的整体进行更新。这就导致看似只是修改了 `x.a` ，但其实又修改了 `x.b`。不过这在 Intel 的 X86 架构上应该是不会出现这种问题的。但是需要注意一点，我们在之前优化技巧中提到了位域语法，使用这个的话就需要注意 Race 的问题了。

这里再补充一点。传统地解决 race 的方法是加锁，但这样会有性能问题。在一些情况下我们可以不用加锁，改用一个叫做 Reducer 的东西。

Reducer 的大概思想就是，如果多个线程需要操作同一个全局变量，那么我们把这个全局变量”局部化“，让线程的操作仅自己可见。然后再最后同步的时候再把各个线程对这个全局变量的操作进行合并。但是 Reducer 有两点局限性：首先它要求操作必须有结合律，其次要求线程不需要获取全局变量当前的值。

### 3.3 Scheduling

现在比较好的一个任务调度策略是 Work Stealing。大概的思想是，每个核心都维护一个 deque，表示当前的任务队列。如果当前任务产生了新的任务，那么就把新任务放入队列的头部，执行这个新任务。如果当前队列的任务都执行完了，那么就去考虑从其他队列的尾部 steal 一些任务过来，使得自己永远保持忙碌。

这个策略的耗时大概是 $T_P \approx \frac{T_1}{P} + O(T_{\infin})$。简单的证明就是，所有核的耗时加起来一定是 $T_1$ 再加上 steal 产生的开销。每次 steal 都有 $\frac{1}{P}$ 的概率 steal 到 critical path 上的任务，使得 span 减一。那么全部的 steal 开销期望就是 $O(PT_{\infin})$。这些时间能分摊到 $P$ 个核心上，所以耗时就是：

$$
\frac{T_1+O(PT_{\infin})}{P} = \frac{T_1}{P} + O(T_{\infin})
$$

我们也能感性地去考虑为什么这个策略很优。首先，一个基本的思想是，你让所有的核心都在不停地执行任务，这样利用率肯定好。沿着这个思路，大概有两种实现，一种是这里的 work stealing，另一种是，如果有新任务产生了，那么我们考虑把这个新任务分给当前压力最小的队列中去。但后者这么做的话，每次产生一个任务都需要一个分配任务的开销，但 work stealing 的策略只会在出现无事可做之后才会去产生分配的开销。而且后者把任务放到其他队列的时候需要加锁，但 work stealing 可以把锁的开销尽可能减小甚至无锁。（参考论文 _The Implementation of the Cilk-5 Multithreaded Language_ ，链接 https://pages.cs.wisc.edu/~swilson/cilk.pdf 第 7 页。不过论文中似乎只讨论了 Thief 和 Worker 的竞争而没说多个 Thief 抢一个 Worker 该怎么搞）
